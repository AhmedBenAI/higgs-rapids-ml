{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2adce54-aed0-4a63-a621-1db88da91cd1",
   "metadata": {},
   "source": [
    "If you want to see the graphs, set show_graphs to True. \n",
    "\n",
    "However, if the kernel crashes, please set it to False to avoid the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a2853-4703-45a6-a95d-796e711abb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888b704-526d-4c3a-b543-f5ade6c45539",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a871c2f-4992-4a1f-a675-b05d5c147fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General-purpose libraries\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU-based libraries\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from cuml.preprocessing import StandardScaler, PowerTransformer, MinMaxScaler\n",
    "from cuml.decomposition import PCA\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.model_selection import GridSearchCV as cuml_GridSearchCV\n",
    "\n",
    "# CPU-based libraries\n",
    "from sklearn.ensemble import RandomForestClassifier as skRF\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# XGBoost (supports both CPU and GPU)\n",
    "import xgboost as xgb\n",
    "\n",
    "# Evaluation libraries\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve,roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e45a98-de1c-48a8-adc4-44bec8f146a1",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e7ae6-7e38-44e0-9ab0-c2c4ce5c0561",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpu=cudf.read_csv(\"Partical.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675405f3-a4b7-48a0-98b0-71cac4ac835b",
   "metadata": {},
   "source": [
    "## Data info and description \r\n",
    "\r\n",
    "We see that the dataset contains1000 1000 observations and the dataset hav28 9 classes and a target class. \r\n",
    "\r\n",
    "We also see in the result below that the dataset contains only float type lues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb23d39d-0a0b-40c6-b9a8-98e6b266ebad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_discription(df_gpu):\n",
    "    print(\"Basic Info\")\n",
    "    print(df_gpu.info())\n",
    "    print(\"\\nSummary Statistics\")\n",
    "    print(df_gpu.describe())\n",
    "data_discription(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0159f05-9cbe-4215-8ca4-8d5b8f4f8150",
   "metadata": {},
   "source": [
    "## Missing Data \n",
    "The dataset dont containe any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95456c-2dca-4e2d-987d-a882a4b0d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(df_gpu):\n",
    "    # Count total missing values in the dataset\n",
    "    total_missing = df_gpu.isnull().sum().sum()\n",
    "\n",
    "    # Count the number of rows with at least one missing value\n",
    "    rows_with_missing = df_gpu.isnull().any(axis=1).sum()\n",
    "\n",
    "    # Display the results\n",
    "    print(f\"Total missing values: {total_missing}\")\n",
    "    print(f\"Rows with missing values: {rows_with_missing}\")\n",
    "missing_data(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd621c-df85-4d78-aa98-1cf64baec45b",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "We have a balanced dataset with a 53:47 ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9acb008-5e41-43a4-bef7-8764504fa908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_distribution(df_gpu):\n",
    "    plt.hist(df_gpu[\"Target\"], bins=2, color='skyblue', edgecolor='black')\n",
    "    plt.xticks([0.25, 0.75], [\"background\", \"signal\"])\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Class Distribution\")\n",
    "    plt.show()\n",
    "if show_graphs:\n",
    "    class_distribution(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d8866-2b0d-48df-88bb-2f296642021b",
   "metadata": {},
   "source": [
    "## Data Distribution \n",
    "Several features in the dataset exhibit left-skewed distributions, including lepton_pT, missing_energy_magnitude, jet_1_pT, jet_2_pT, jet_3_pT, jet_4_pT, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, and m_wwbb. Additionally, the dataset shows a wide range of feature values, for example, missing_energy_magnitude and various jet transverse momentum (pT) values are significantly larger compared to other features. This inconsistency in value ranges could cause certain features to dominate the learning process, leading to suboptimal model performance. Although the remaining features generally follow a bell-shaped distribution, ideal for training, inconsistencies across feature scales still pose a challenge for effective model learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309518c5-e85e-4b66-b4d0-a903a3e85501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_data_disp(df_gpu):\n",
    "    num_features = len(df_gpu.columns)\n",
    "    num_rows = 8  # Adjusted to fit 28 features\n",
    "    num_cols = 4  \n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 28))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Iterate through columns to create histograms\n",
    "    for i, column in enumerate(df_gpu.columns):\n",
    "        axes[i].hist(df_gpu[column], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Histogram of {column}')\n",
    "        axes[i].set_xlabel(column)\n",
    "        axes[i].set_ylabel('Count')\n",
    "    \n",
    "    # Hide any unused subplots (if total columns < grid size)\n",
    "    for j in range(num_features, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if show_graphs:\n",
    "    display_data_disp(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca96edc-cdbb-4291-b31d-18f7848932c4",
   "metadata": {},
   "source": [
    "## Outliers\n",
    "the dataset has many outliers across different features. However, we can expect that most outliers are due to skewness left in the dataset, so by fixing the skewing in the features, most of the outliers will be delt with. For the remaining outliers we need to use imputation to remove them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073f834a-abe0-4dc0-9caf-6eb5da85ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_outliers(df_gpu):\n",
    "    numeric_columns = df_gpu.select_dtypes(include='number').columns\n",
    "    num_features = len(numeric_columns)\n",
    "\n",
    "    num_rows = 8  # Adjusted to fit 28 features\n",
    "    num_cols = 4  \n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(16, 28))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    # Flatten axes for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Iterate through numeric columns to create box plots\n",
    "    for i, column in enumerate(numeric_columns):\n",
    "        axes[i].boxplot(df_gpu[column], vert=False, patch_artist=True, boxprops=dict(facecolor='skyblue'))\n",
    "        axes[i].set_title(f'Box Plot of {column}')\n",
    "        axes[i].set_xlabel(column)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for j in range(num_features, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "if show_graphs:\n",
    "    display_outliers(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ecbdf-e685-49c1-a162-d27fe191d99c",
   "metadata": {},
   "source": [
    "## Correlation \n",
    "The dataset is fairly not correlated except for m_wwbb and m_wbb. which are highly correlated wich suggest removing one since they containe similare information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e9fd7-2cb7-47ff-b2b5-80fb141f6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(df_gpu):\n",
    "    corr_matrix = df_gpu.to_pandas().corr()  # Convert only for correlation computation\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(corr_matrix, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
    "    plt.colorbar()\n",
    "\n",
    "    num_vars = len(corr_matrix.columns)\n",
    "    for i in range(num_vars):\n",
    "        for j in range(num_vars):\n",
    "            plt.text(j, i, f\"{corr_matrix.iloc[i, j]:.1f}\", \n",
    "                     ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    plt.xticks(range(num_vars), corr_matrix.columns, rotation=90)\n",
    "    plt.yticks(range(num_vars), corr_matrix.columns)\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "    plt.show()\n",
    "\n",
    "if show_graphs:\n",
    "    correlation(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c20bed-8923-458e-b3c9-37480a1cf2fe",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf68da6-513a-436e-af15-6fa9d73d2327",
   "metadata": {},
   "source": [
    "In this section we will remove m_wwbb since it highly correlated with other features and we will fix the distrebution using Power Transformer and MinMaxScaler to get a set range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a296d33e-8fee-4aef-adbe-20598e0a1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_left_skewness_gpu(df, features):\n",
    "    \"\"\"\n",
    "    Applies a PowerTransformer (Yeo-Johnson) to fix left-skewed features in a GPU-accelerated dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (cudf.DataFrame): Input dataframe.\n",
    "        features (list): List of feature names to transform.\n",
    "\n",
    "    Returns:\n",
    "        cudf.DataFrame: Transformed dataframe with reduced skewness.\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    \n",
    "    # Apply PowerTransformer to the selected features\n",
    "    df_transformed[features] = pt.fit_transform(df_transformed[features])\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def normalize_data_gpu(df, batch_size=1_000_000):\n",
    "    \"\"\"\n",
    "    Normalizes all features in the dataframe to the [0, 1] range using MinMaxScaler,\n",
    "    processing in batches to handle large datasets on GPU.\n",
    "\n",
    "    Args:\n",
    "        df (cudf.DataFrame): Input dataframe to normalize.\n",
    "        batch_size (int): Number of rows to process in each batch.\n",
    "\n",
    "    Returns:\n",
    "        cudf.DataFrame: Normalized dataframe.\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Fit the scaler on the full dataset\n",
    "    scaler.fit(df)\n",
    "    \n",
    "    # Initialize an empty dataframe for storing normalized batches\n",
    "    df_normalized = cudf.DataFrame()\n",
    "    \n",
    "    # Manually setting number of batches, adjust as needed\n",
    "    num_batches = 11  \n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, len(df))\n",
    "        \n",
    "        # Extract a batch of data\n",
    "        batch = df.iloc[start:end]\n",
    "        \n",
    "        # Normalize the batch\n",
    "        normalized_batch = scaler.transform(batch)\n",
    "        \n",
    "        # Preserve the original column names\n",
    "        normalized_batch.columns = df.columns\n",
    "        \n",
    "        # Append the normalized batch to the output dataframe\n",
    "        df_normalized = cudf.concat([df_normalized, normalized_batch], ignore_index=True)\n",
    "    \n",
    "    return df_normalized\n",
    "    \n",
    "def set_target_variable(df):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into features (X) and target variable (y).\n",
    "\n",
    "    Args:\n",
    "        df (cudf.DataFrame): Input dataframe with a 'Target' column.\n",
    "\n",
    "    Returns:\n",
    "        X (cudf.DataFrame): Feature dataframe.\n",
    "        y (cudf.Series): Target variable.\n",
    "    \"\"\"\n",
    "    X = df.drop('Target', axis=1)\n",
    "    y = df['Target']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb49c961-2db1-4014-9c2c-0ac79b0fda59",
   "metadata": {},
   "source": [
    "## Set Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e04ad5-7fe7-4f05-9ea0-8b6cd6733e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=set_target_vriable(df_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fad78-7292-4aef-aaba-f21e69a8bb91",
   "metadata": {},
   "source": [
    "## Remove Highly Correlated Fearures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0cd95-4486-491d-a17d-eeb5110f5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop('m_wwbb', axis=1)  # Drop 'm_wwbb' feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb816336-4f43-495b-a139-3f12adc028cc",
   "metadata": {},
   "source": [
    "## Fix Distrebution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6917568-1923-4184-90f5-0bea2428deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = [\n",
    "    \"lepton_pT\", \"missing_energy_magnitude\", \"jet_1_pt\", \"jet_2_pt\", \"jet_3_pt\", \"jet_4_pt\", \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdb045-af1f-4432-b795-cc7e627a4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=fix_left_skewness_gpu(X, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4028b-16bb-4ceb-a1a6-62cb07c9b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=normalize_data_gpu(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35f90d3-1723-445c-85cd-6ac8df5424e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    display_data_disp(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd99096-f608-45c8-a867-74b51f48e870",
   "metadata": {},
   "source": [
    "# 4. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db107a-6118-42a8-a7e6-e25ddeb094bd",
   "metadata": {},
   "source": [
    "In this section we will split the dataset into 3 sets:\n",
    " - train 80% used to train the models \n",
    " - test 10% test the prediction of the models in the training process\n",
    " - validation 10% used to evaluate the models\n",
    "\n",
    "after that we are going to use 2 metods of Dimensionality Reduction:\n",
    "\n",
    " - PCA\n",
    " - feature Importance\n",
    "\n",
    "we will compare the result of both methods and conclude which one is best for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd022d33-ed20-4e06-9355-ca3ca5aa7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, train_frac=0.8, val_frac=0.1, test_frac=0.1, random_state=42):\n",
    "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-6, \"Fractions must sum to 1.\"\n",
    "\n",
    "    n_samples = len(X)\n",
    "    cp.random.seed(random_state)\n",
    "    indices = cp.random.permutation(n_samples)\n",
    "\n",
    "    train_end = int(train_frac * n_samples)\n",
    "    val_end = train_end + int(val_frac * n_samples)\n",
    "\n",
    "    train_idx = indices[:train_end]\n",
    "    val_idx = indices[train_end:val_end]\n",
    "    test_idx = indices[val_end:]\n",
    "\n",
    "    # Convert to cuDF Series for indexing\n",
    "    train_idx = cudf.Series(train_idx)\n",
    "    val_idx = cudf.Series(val_idx)\n",
    "    test_idx = cudf.Series(test_idx)\n",
    "\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db88363e-a2b0-4b01-80c6-e9f178db39d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_PCA(X, n_components=None, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Performs PCA on large GPU-based data with optional automatic component selection.\n",
    "\n",
    "    Parameters:\n",
    "        X (cuDF.DataFrame): Feature matrix (excluding target column).\n",
    "        n_components (int, optional): Number of PCA components. If None, it will be chosen based on variance_threshold.\n",
    "        variance_threshold (float, optional): Target cumulative variance if n_components is None.\n",
    "\n",
    "    Returns:\n",
    "        X_pca_gpu (cp.ndarray): Transformed data.\n",
    "        explained_variance (np.ndarray): Explained variance ratio for each component.\n",
    "        pca (cuml.PCA): Trained PCA model.\n",
    "    \"\"\"\n",
    "    batch_size = 500000\n",
    "    num_batches = (X.shape[0] // batch_size)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    scaled_chunks = []\n",
    "\n",
    "    # Process data in batches\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, X.shape[0])\n",
    "        X_batch = X.iloc[start:end, :]  # Select the features only\n",
    "\n",
    "        # Scale the batch (GPU-based scaling)\n",
    "        X_scaled_batch = scaler.fit_transform(X_batch).to_cupy()\n",
    "        scaled_chunks.append(X_scaled_batch)\n",
    "\n",
    "    # Concatenate all scaled batches into one array\n",
    "    X_scaled_gpu = cp.concatenate(scaled_chunks, axis=0)\n",
    "\n",
    "    if n_components is None:\n",
    "        # Temporarily fit with all components to calculate cumulative variance\n",
    "        pca_temp = PCA()\n",
    "        X_temp = pca_temp.fit_transform(X_scaled_gpu)\n",
    "        cum_var = cp.cumsum(pca_temp.explained_variance_ratio_).get()\n",
    "        \n",
    "        # Select the number of components based on the variance threshold\n",
    "        n_components = np.argmax(cum_var >= variance_threshold) + 1\n",
    "        print(f\"üìå Automatically selected n_components = {n_components} to retain {variance_threshold*100:.0f}% variance.\")\n",
    "\n",
    "    # Final PCA with the selected number of components\n",
    "    start_time = time.time()\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca_gpu = pca.fit_transform(X_scaled_gpu)\n",
    "    end_time = time.time()\n",
    "\n",
    "    explained_variance = pca.explained_variance_ratio_.get()\n",
    "\n",
    "    # Plot cumulative explained variance\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(explained_variance) + 1), explained_variance.cumsum(), marker='o', linestyle='--', color='b')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA Explained Variance (GPU)')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ PCA completed on GPU.\")\n",
    "    print(f\"üîπ Total Explained Variance: {explained_variance.sum():.4f}\")\n",
    "    print(f\"üîπ Time Taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"üîπ PCA Output Shape: {X_pca_gpu.shape}\")\n",
    "    \n",
    "    # Convert to cuDF DataFrame\n",
    "    X_pca_cudf = cudf.DataFrame(X_pca_gpu, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "\n",
    "    return X_pca_cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021a876-5126-441e-8338-da0f844edb8d",
   "metadata": {},
   "source": [
    "## PCA \n",
    "Using PCA we have been able to reduce the number of fetures to 23 and keep a CEV of 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64646675-538a-4e59-9adb-f937f8edc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_x=perform_PCA(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236cadf-03bc-43ae-a11d-72ce23eca418",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train_pca, X_val_pca, X_test_pca, y_train_pca, y_val_pca, y_test_pca=split_dataset(pca_x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd7387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X):\n",
    "    X_filtered=X\n",
    "    # Ensure X_filtered and y are cudf.DataFrames\n",
    "    X_filtered_pd = X_filtered.to_pandas()\n",
    "    y_pd = y.to_pandas()\n",
    "    \n",
    "    # Train XGBoost model on GPU\n",
    "    model = xgb.XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "    model.fit(X_filtered_pd, y_pd)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Convert to cudf for efficient GPU operations\n",
    "    feature_importance_df = cudf.DataFrame({\n",
    "        \"feature\": X_filtered.columns,\n",
    "        \"importance\": importances\n",
    "    })\n",
    "    \n",
    "    importances_list = importances.tolist()\n",
    "    \n",
    "    # Set threshold dynamically (e.g., keep top N% instead of fixed 0.01)\n",
    "    top_n_percent = 0.2  # Keep top 20% most important features\n",
    "    threshold = np.percentile(importances, (1 - top_n_percent) * 100)\n",
    "    important_features = [f for f, score in zip(X_filtered.columns, importances_list) if score >= threshold]\n",
    "    \n",
    "    \n",
    "    # Select only important features\n",
    "    X_important = X_filtered[important_features]\n",
    "    print(important_features)\n",
    "    return X_important\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc859f4-0fb5-4fac-9c5c-320117a19cb6",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "\n",
    "Using the top 20% feature which are missing_energy_magnitude, jet_1_pt, m_jjj, m_jlv, m_bb, m_wbb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1c1a7-0977-4a26-ac59-8b4e25d69e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=feature_importance(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f7320-0c50-4c34-a8c4-0311df18f034",
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_val, X_test, y_train, y_val, y_test=split_dataset(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44725f7c-c44a-48b6-bfc0-18c76a58e4fb",
   "metadata": {},
   "source": [
    "# 5. GPU-Based Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aba854-e6e9-4939-8989-7b4cd961e9b3",
   "metadata": {},
   "source": [
    "In this section we will train 2 models XGboost and Rendom Forest using both reduced dataset from PCA and feature Importance using GPU and CPU to see which is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d8913-5c58-4348-8997-b03134049985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_gpu(X_train, y_train, X_val, y_val):\n",
    "    results = {}\n",
    "    # Convert target to pandas for CPU models\n",
    "    y_train_cpu = y_train.to_pandas()\n",
    "    y_val_cpu = y_val.to_pandas()\n",
    "\n",
    "    xgb_gpu_params = {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "    xgb_gpu_params = xgb_gpu_params.copy()\n",
    "    xgb_gpu_params['tree_method'] = 'gpu_hist'\n",
    "    # GPU XGBoost\n",
    "    start = time.time()\n",
    "    xgb_gpu = xgb.XGBClassifier(**xgb_gpu_params)\n",
    "    xgb_gpu.fit(X_train.to_pandas(), y_train_cpu)\n",
    "    duration = time.time() - start\n",
    "    acc = xgb_gpu.score(X_val.to_pandas(), y_val_cpu)\n",
    "    results['XGBoost GPU'] = (duration, acc)\n",
    "    \n",
    "    rf_gpu_params = {\n",
    "        'n_estimators': 50,\n",
    "        'max_depth': 8,\n",
    "        'bootstrap': True,\n",
    "        'random_state': 42\n",
    "        # Note: cuML RF doesn't support max_features\n",
    "    }\n",
    "\n",
    "    # GPU RF\n",
    "    start = time.time()\n",
    "    rf_gpu = cuRF(**rf_gpu_params)\n",
    "    rf_gpu.fit(X_train, y_train)\n",
    "    preds = rf_gpu.predict(X_val)\n",
    "    duration = time.time() - start\n",
    "    acc = (preds == y_val).mean()\n",
    "    results['Random Forest GPU'] = (duration, acc.item())\n",
    "\n",
    "    # ====================== Results =========================\n",
    "    print(\"\\nüîç Training Time and Accuracy Summary:\")\n",
    "    for model, (time_taken, accuracy) in results.items():\n",
    "        print(f\"{model:<20} | ‚è± {time_taken:.2f}s | üéØ Accuracy: {accuracy:.4f}\")\n",
    "    models=[rf_gpu,xgb_gpu]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e8a7d-6b10-4f20-9898-37830b771d00",
   "metadata": {},
   "source": [
    "## GPU + Feature Importance\n",
    "We can see that the models have trained super quickly and we obtaind some pretty solid results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204bb9b9-0662-45cc-bcbc-744d790cde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training + timing\n",
    "gpu_models = train_models_gpu(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209f24ba-15d3-468e-8071-35d74ef471cc",
   "metadata": {},
   "source": [
    "## GPU + PCA\n",
    "Model training took a bit longer but still fast. the result are worst then the XGBoost with a dorp on speed and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89369411-08f7-43f0-9ae0-2b441262850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training + timing\n",
    "gpu_models_pca = train_models_gpu(X_train_pca, y_train_pca, X_val_pca, y_val_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63315e-1eea-4ba8-a028-46ba3d8354aa",
   "metadata": {},
   "source": [
    "### When we used 40% of the top features (9 features) in the feature importance we got:\n",
    "\n",
    "üîç Training Time and Accuracy Summary:\n",
    "\r\n",
    "XGBoost GPU          | ‚è± 9.08s | üéØ Accuracy: 0.728\n",
    "\n",
    "3\r\n",
    "Random Forest GPU    | ‚è± 16.07s | üéØ Accuracy: 0.6883"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a204627-bd56-4a81-81ba-ded253caf082",
   "metadata": {},
   "source": [
    "# 5. CPU-Based Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe0354-3a64-491c-94f5-af338278d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_cpu(X_train, y_train, X_val, y_val):\n",
    "    results = {}\n",
    "    xgb_cpu_params = {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    # Convert target to pandas for CPU models\n",
    "    y_train_cpu = y_train.to_pandas()\n",
    "    y_val_cpu = y_val.to_pandas()\n",
    "\n",
    "    # ====================== XGBoost =========================\n",
    "\n",
    "    # CPU XGBoost\n",
    "    start = time.time()\n",
    "    xgb_cpu = xgb.XGBClassifier(**xgb_cpu_params)\n",
    "    xgb_cpu.fit(X_train.to_pandas(), y_train_cpu)\n",
    "    duration = time.time() - start\n",
    "    acc = xgb_cpu.score(X_val.to_pandas(), y_val_cpu)\n",
    "    results['\\n\\nXGBoost CPU'] = (duration, acc)\n",
    "    \n",
    "    # ================= Random Forest ========================\n",
    "    rf_cpu_params = {\n",
    "        'n_estimators': 50,\n",
    "        'max_depth': 8,\n",
    "        'max_features': 'sqrt',\n",
    "        'bootstrap': True,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    # CPU RF\n",
    "    start = time.time()\n",
    "    rf_cpu = skRF(**rf_cpu_params)\n",
    "    rf_cpu.fit(X_train.to_pandas(), y_train_cpu)\n",
    "    duration = time.time() - start\n",
    "    acc = rf_cpu.score(X_val.to_pandas(), y_val_cpu)\n",
    "    results['Random Forest CPU'] = (duration, acc)\n",
    "    \n",
    "   # ====================== Results =========================\n",
    "    print(\"\\nüîç Training Time and Accuracy Summary:\")\n",
    "    for model, (time_taken, accuracy) in results.items():\n",
    "        print(f\"{model:<20} | ‚è± {time_taken:.2f}s | üéØ Accuracy: {accuracy:.4f}\")\n",
    "    models=[rf_cpu,xgb_cpu]\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d355270-e6f8-4659-aa44-cc9c82839bc5",
   "metadata": {},
   "source": [
    "## CPU + Feature Importance\n",
    "The models took too long to train using the small set of features and the results are worst.\n",
    "\n",
    "Training the models using the PCA dataset will be too long and will result in lower accuracy so we are not going to do it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c1a87-875c-4580-afbc-caf715ff1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_models = train_models_cpu(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf72c4a-97a8-470d-9eda-99303add4bf1",
   "metadata": {},
   "source": [
    "# 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a19ae-da70-42ac-8b4b-db7a594d5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_gpu(X_test, y_test, model_list):\n",
    "    # Convert cuDF to NumPy arrays (only once, at the beginning)\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    # Model names for printing and labeling plots\n",
    "    model_names = [\"XGBoost\", \"RandomForest\"][:len(model_list)]\n",
    "\n",
    "    for i, model in enumerate(model_list):\n",
    "        name = model_names[i]\n",
    "        y_pred = model.predict(X_test_np)\n",
    "        y_prob = model.predict_proba(X_test_np)[:, 1]\n",
    "\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_np, y_pred))\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test_np, y_pred))\n",
    "\n",
    "        auc = roc_auc_score(y_test_np, y_prob)\n",
    "        print(f\"ROC-AUC: {auc:.2f}\")\n",
    "\n",
    "        # ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test_np, y_prob)\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.2f})\")\n",
    "\n",
    "    # Final plot formatting\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c1d54-49ce-49d6-9faf-24b61083fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_gpu(X_val_pca, y_val_pca,gpu_models_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b2308-fb35-4828-b5d9-b2e2ae1dd450",
   "metadata": {},
   "source": [
    "## GPU vs CPU results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1327d6c2-4b2d-4a8b-aa5c-debea6e458ff",
   "metadata": {},
   "source": [
    "The results produced by the GPU-trained models are identical to those obtained using CPU-based models. However, training on the GPU is significantly faster ‚Äî approximately 10√ó faster for XGBoost and 75√ó faster for Random Forest.\n",
    "\n",
    "Based on these results, we can conclude that using a CPU offers no practical advantage over GPU training. Additionally, while XGBoost shows a slightly lower AUC score (by just 0.02), its dramatically faster training time makes it the more suitable choice for this task.\n",
    "\n",
    "When evaluating feature selection techniques, it was found that using 20% Feature Importance (FI) performed better overall. Although using 40% FI slightly improved the model's accuracy by 0.02, it introduced 5 additional features, increasing computational cost without delivering a proportionate performance benefit. Furthermore, the use of Principal Component Analysis (PCA) was less effective in this case, the PCA transformation resulted in 23 components, and the model's performance was noticeably worse compared to using 20% FI-selected features. Therefore, selecting the top 20% most important features offers the best balance between model performance and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcfa022-4772-45a5-bde0-f253c52a11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_gpu(X_val, y_val,gpu_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ee6335-a81d-4f3a-9756-39ace8894691",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_gpu(X_val, y_val,cpu_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
